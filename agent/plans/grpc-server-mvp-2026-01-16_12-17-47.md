# gRPC Server Implementation Plan - MVP (Phase 1)

**Created:** 2026-01-16
**Scope:** Implement gRPC server to enable client-server communication
**Target:** RunPod GPU pods

---

## Executive Summary

The client (`client.py`) already implements gRPC calls via `TensorInferenceStub`. The server (`server.py`) only has an HTTP handler. This plan adds a gRPC servicer to bridge the gap.

---

## Prerequisites & Dependencies

### Already Complete
- [x] Proto file defined (`tensor_service.proto`)
- [x] gRPC stubs generated (`tensor_service_pb2.py`, `tensor_service_pb2_grpc.py`)
- [x] Client gRPC implementation (`client.py:_call_server()`)
- [x] Dependencies declared (`grpcio>=1.66.0`, `grpcio-tools>=1.66.0`)
- [x] Configuration exists (`ServerSettings.grpc_port = 50051`)
- [x] Core inference logic (`forward_transformer()`)

### Blockers
- None identified. All dependencies are in place.

---

## Task List

### Task 1: Implement TensorInferenceServicer Class

**File:** `infemeral/server.py`
**Estimated Lines:** ~80

```python
class TensorInferenceServicer(tensor_service_pb2_grpc.TensorInferenceServicer):
    def __init__(self, model, device: str = "cuda"):
        self.model = model
        self.device = device

    def Infer(self, request, context) -> tensor_service_pb2.InferenceResponse:
        # Implementation here
```

**Subtasks:**
1. Import gRPC servicer base class from generated stubs
2. Create `TensorInferenceServicer` class
3. Implement `Infer()` method:
   - Extract fields from `request` (cloaked_embedding, encrypted_session_key, nonce, shape, dtype, session_id)
   - Decrypt cloaked embedding using `decrypt_bytes()`
   - Deserialize tensor using `deserialize_tensor()`
   - Load KV cache via `load_kv_cache()`
   - Call `forward_transformer()`
   - Save updated KV cache via `save_kv_cache()`
   - Serialize output using `serialize_tensor()`
   - Encrypt output using `encrypt_bytes()`
   - Return `InferenceResponse` with output, shape, dtype, tokens_processed

**Error Handling:**
- Wrap logic in try/except
- Set `response.error` on exception
- Use `context.set_code()` and `context.set_details()` for gRPC status

---

### Task 2: Implement gRPC Server Function

**File:** `infemeral/server.py`
**Estimated Lines:** ~40

```python
def serve_grpc(port: int = None, max_workers: int = 4):
    """Start the gRPC inference server."""
```

**Subtasks:**
1. Load model using existing `load_model()` function
2. Create `ThreadPoolExecutor` with configurable workers
3. Create gRPC server with large message options:
   ```python
   options=[
       ('grpc.max_send_message_length', 100 * 1024 * 1024),
       ('grpc.max_receive_message_length', 100 * 1024 * 1024),
   ]
   ```
4. Instantiate `TensorInferenceServicer` with loaded model
5. Register servicer using `add_TensorInferenceServicer_to_server()`
6. Bind to port from `server_settings.grpc_port` (default 50051)
7. Add graceful shutdown handling (SIGTERM, SIGINT)
8. Call `server.start()` and `server.wait_for_termination()`

---

### Task 3: Add CLI Entry Point

**File:** `infemeral/server.py`
**Estimated Lines:** ~15

```python
if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--port", type=int, default=None)
    parser.add_argument("--workers", type=int, default=4)
    args = parser.parse_args()
    serve_grpc(port=args.port, max_workers=args.workers)
```

**Alternative:** Add to `pyproject.toml` as console script:
```toml
[project.scripts]
infemeral-server = "infemeral.server:serve_grpc"
```

---

### Task 4: Integration Test

**File:** `tests/test_grpc_integration.py`
**Estimated Lines:** ~100

**Subtasks:**
1. Create pytest fixture that starts gRPC server in background thread
2. Create test client that connects to server
3. Test cases:
   - `test_single_token_inference`: Send cloaked embedding, verify response shape
   - `test_session_isolation`: Multiple sessions don't share KV cache
   - `test_error_handling`: Malformed request returns error in response
   - `test_large_batch`: Verify 100MB message limits work

**Fixture pattern:**
```python
@pytest.fixture(scope="module")
def grpc_server():
    server_thread = threading.Thread(target=serve_grpc, kwargs={"port": 50052})
    server_thread.daemon = True
    server_thread.start()
    time.sleep(2)  # Wait for server startup
    yield "localhost:50052"
```

---

### Task 5: Remove Debug Logging from forward_transformer()

**File:** `infemeral/server.py`
**Location:** Lines ~230-450

**Subtasks:**
1. Remove or comment out DEBUG print statements
2. Replace with proper `logging.debug()` calls if needed
3. Ensure INFO-level logging for important events only

---

## Impacted Files

| File | Change Type | Description |
|------|-------------|-------------|
| `infemeral/server.py` | Modified | Add TensorInferenceServicer, serve_grpc() |
| `tests/test_grpc_integration.py` | New | Integration tests |
| `pyproject.toml` | Modified (optional) | Add console script entry point |

---

## Risk Assessment

| Risk | Impact | Mitigation |
|------|--------|------------|
| Model loading slow on first request | Medium | Pre-load model in `serve_grpc()` before accepting connections |
| Memory leak from KV cache accumulation | Medium | Document session cleanup; defer to Phase 2 |
| gRPC timeout on large models | Medium | Configure appropriate deadlines; document expected latency |
| Thread safety of global model | Low | PyTorch inference is thread-safe; one model instance is fine |

---

## Success Criteria

### Definition of Done
- [ ] `python -m infemeral.server --port 50051` starts gRPC server
- [ ] Client can complete single-token inference via gRPC
- [ ] `pytest tests/test_grpc_integration.py` passes
- [ ] No debug print statements in production code path
- [ ] Server handles malformed requests gracefully (returns error, doesn't crash)

### Validation Steps
1. Start server: `python -m infemeral.server`
2. Run client test: `pytest tests/test_grpc_integration.py -v`
3. Manual smoke test with `test_input.json` converted to gRPC call
4. Verify server logs show request processing

---

## Out of Scope (Deferred to Phase 2)

- KV cache format conversion for multi-turn (separate plan)
- TLS/secure transport
- RSA-OAEP session key wrapping
- Session expiration/cleanup
- Health check endpoint
- Metrics/observability

---

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                         Client (Python)                          │
├─────────────────────────────────────────────────────────────────┤
│  1. Tokenize prompt (local)                                      │
│  2. Embed tokens → hidden states (local, embed_tokens)           │
│  3. Cloak: x' = (x + noise) @ M.T (local)                        │
│  4. Encrypt: AES-256-GCM(serialize(x')) (local)                  │
│  5. Build InferenceRequest protobuf                              │
└───────────────────────────┬─────────────────────────────────────┘
                            │ gRPC Infer()
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│                    gRPC Server (RunPod)                          │
├─────────────────────────────────────────────────────────────────┤
│  TensorInferenceServicer.Infer():                                │
│  1. Decrypt cloaked embedding                                    │
│  2. Deserialize tensor                                           │
│  3. Load KV cache (if session exists)                            │
│  4. forward_transformer(model, hidden_states, past_kv)           │
│  5. Save KV cache                                                │
│  6. Serialize output                                             │
│  7. Encrypt output                                               │
│  8. Return InferenceResponse                                     │
└───────────────────────────┬─────────────────────────────────────┘
                            │ InferenceResponse
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│                         Client (Python)                          │
├─────────────────────────────────────────────────────────────────┤
│  6. Decrypt response                                             │
│  7. Deserialize tensor                                           │
│  8. Uncloak: x = x' @ M (local)                                  │
│  9. Compute logits: logits = x @ lm_head.T (local)               │
│  10. Sample next token (local)                                   │
│  11. Repeat from step 2 for next token                           │
└─────────────────────────────────────────────────────────────────┘
```

---

## Estimated Effort

| Task | Complexity | Notes |
|------|------------|-------|
| Task 1: Servicer class | Medium | Core logic exists in handler(), need to adapt |
| Task 2: serve_grpc() | Low | Standard gRPC server boilerplate |
| Task 3: CLI entry point | Low | Simple argparse |
| Task 4: Integration tests | Medium | Need mock model or skip model load |
| Task 5: Remove debug logs | Low | Find and delete/replace |

---

## Next Steps After Completion

1. **Phase 2 Plan:** KV cache multi-turn support (separate document)
2. **Phase 3 Plan:** Security hardening (RSA wrapping, TLS)
